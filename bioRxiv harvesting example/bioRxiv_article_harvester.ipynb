{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Script to open, download, and parse every article page on bioRxiv\n",
    "# specified in the file biorxiv_dois.txt (this should be all of them),\n",
    "# extract relevant information, write it to a file, and download\n",
    "# the earliest and latest uploaded preprint PDFs of the paper.\n",
    "\n",
    "import codecs # Helps with character encodings\n",
    "from selenium import webdriver # Web browser automation tools\n",
    "from selenium.webdriver.common.keys import Keys # ditto\n",
    "from bs4 import BeautifulSoup as bs # HTML parser\n",
    "from slugify import slugify # Turns strings into nice filenames\n",
    "import pickle # Used to save data (anything!) to a file and get it back later\n",
    "import time # Includes the sleep() command, which pauses for X seconds\n",
    "import os # Useful for file system operations\n",
    "import sys # Random system functions\n",
    "import urllib # Another library for downloading stuff from the web\n",
    "import operator # Useful for sorting data in ascending or descing order\n",
    "\n",
    "# Make sure we're using UTF-8\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "# Set to True to re-download all PDFs\n",
    "downloadPapers = False\n",
    "\n",
    "# Open some files for output, logging, etc.\n",
    "allfile = codecs.open(\"biorxiv_all_dois.txt\", 'w', 'utf-8')\n",
    "outfile = codecs.open(\"biorxiv_published_dois.txt\", 'w', 'utf-8')\n",
    "smallout = codecs.open(\"biorxiv_pub_dois.txt\", 'w', 'utf-8')\n",
    "logfile = codecs.open(\"scrape_log.txt\", 'w', 'utf-8')\n",
    "\n",
    "journalCounts = {}\n",
    "\n",
    "def cacheQuery(query, forceUncache=False):\n",
    "    \n",
    "  # Function to open, download and cache (pickle) a specified website.\n",
    "  # If the site is already cached, just return it.\n",
    "  # If forceUncache is True, re-download the page even if it's already cached.\n",
    "\n",
    "  print \"querying \" + query\n",
    "  queryFile = 'articlePages/' + slugify(query)\n",
    "  if ((not forceUncache) and os.path.isfile(queryFile)):\n",
    "    data = pickle.load(open(queryFile, 'rb'))\n",
    "  else:\n",
    "    # Play nice\n",
    "    time.sleep(1)\n",
    "    # Start the mind-controlled browser\n",
    "    chromedriver = \"/usr/bin/chromedriver\"\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "    browser = webdriver.Chrome(chromedriver)\n",
    "    # Open the page in the browser\n",
    "    browser.get(query)\n",
    "    # Get the contents of the page\n",
    "    data = browser.page_source\n",
    "    # Save the contents to a file\n",
    "    pickle.dump(data, open(queryFile, 'wb'))\n",
    "    # Close the mind-controlled browser\n",
    "    browser.quit()\n",
    "  return data\n",
    "\n",
    "def processArticlePage(page, query, bxDOI):\n",
    "    \n",
    "  # Function to parse an article page, e.g.,\n",
    "  # http://biorxiv.org/content/early/2016/07/22/059899\n",
    "  # given the contents of the page\n",
    "\n",
    "  global outfile, newVersions, publishedDOIs, journalCounts\n",
    "  \n",
    "  # Load the page HTML into the BeautifulSoup parser\n",
    "  html = bs(page, \"lxml\")\n",
    "\n",
    "  # Get the page metadata\n",
    "  meta = html.find('head')\n",
    "\n",
    "  # Find the download link for the latest PDF\n",
    "  downloadLink = \"NONE\"\n",
    "  dLink = meta.find('meta', attrs={'name': 'citation_pdf_url'})\n",
    "  if (dLink is not None):\n",
    "    downloadLink = dLink.get('content')\n",
    "\n",
    "  # Find the article abstract text in the page metadata\n",
    "  abstract = \"NONE\"\n",
    "  absElt = meta.find('meta', attrs={'name': 'DC.Description'})\n",
    "  if (absElt is not None):\n",
    "    abstract = absElt.get('content').replace(\"\\n\", \"\").strip()\n",
    "\n",
    "  # Find the article's publication date (if known) in the metadata\n",
    "  pubDate = \"NONE\"\n",
    "  pubElt = meta.find('meta', attrs={'name': 'DC.Date'})\n",
    "  if (absElt is not None):\n",
    "    pubDate = pubElt.get('content')\n",
    "\n",
    "  content = html.find('section', attrs={'id': 'section-content'})\n",
    "\n",
    "  # If we can't find the download link in the metadata, search for it in the\n",
    "  # content section of the page\n",
    "  if (downloadLink == \"NONE\"):\n",
    "    contentPanes = content.find_all('div', attrs={'class': 'pane-content'})\n",
    "    for pane in contentPanes:\n",
    "      pLink = pane.find('a')\n",
    "      if (pLink is not None):\n",
    "        if (pLink.get('href').find('.full.pdf') >= 0):\n",
    "          downloadLink = pLink.get('href')\n",
    "\n",
    "  # Find the publication journal and DOI\n",
    "  jnlStr = \"NONE\"\n",
    "  doiStr = \"NONE\"\n",
    "  pub_info = content.find('div', attrs={'class': 'pub_jnl'})\n",
    "  if (pub_info is not None):\n",
    "    pubStr = pub_info.text.replace(\"\\n\", \"\").strip()\n",
    "    if (pubStr.find('doi') > 0):\n",
    "      publishedDOIs += 1\n",
    "      pubA = pubStr.split(' doi: ')\n",
    "      doiStr = pubA[-1]\n",
    "      jnlStr = pubA[0].replace(\"Now published in \", \"\")\n",
    "\n",
    "  # Parse the version upload history for the paper\n",
    "  versionsStr = \"NONE\"\n",
    "  earlierVersions = []\n",
    "  versionsPanel = content.find('div', attrs={'class': 'pane-highwire-versions'})\n",
    "  if (versionsPanel is not None):\n",
    "    versions = versionsPanel.find_all('a', attrs={'class': 'hw-version-previous-link'}) \n",
    "    newVersions += 1\n",
    "    for version in versions:\n",
    "      versionLink = version.get('href')\n",
    "      earlierVersions.append(versionLink)\n",
    "    versionsStr = \"|\".join(earlierVersions)\n",
    "\n",
    "  # Write all of the parsed data about the article to various output files\n",
    "  alldata = [bxDOI, pubDate, jnlStr, doiStr, versionsStr]\n",
    "  allfile.write(\"\\t\".join(alldata) + \"\\n\")\n",
    "  if (jnlStr != \"NONE\"):\n",
    "    outdata = [bxDOI, query, downloadLink, pubDate, jnlStr, doiStr, versionsStr, abstract]\n",
    "    smalldata = [bxDOI, jnlStr, doiStr]\n",
    "    outfile.write(\"\\t\".join(outdata) + \"\\n\")\n",
    "    smallout.write(\"\\t\".join(smalldata) + \"\\n\")\n",
    "\n",
    "    if (jnlStr in journalCounts):\n",
    "      journalCounts[jnlStr] += 1\n",
    "    else:\n",
    "      journalCounts[jnlStr] = 1\n",
    "\n",
    "  # Download most recent version as PDF\n",
    "  if (downloadPapers and (downloadLink != \"NONE\")):\n",
    "    latestVersionURL = downloadLink\n",
    "    localDest = \"biorxiv_latest/\" + bxDOI.replace('/', '_') + \".pdf\"\n",
    "    if (not os.path.isfile(localDest)):\n",
    "      try:\n",
    "        print \"Downloading \" + latestVersionURL + \" into \" + localDest\n",
    "        urllib.urlretrieve(latestVersionURL, localDest)\n",
    "        time.sleep(10)\n",
    "      except (urllib.ContentTooShortError, IOError), e:\n",
    "        print \"Error downloading \" + latestVersionURL + \" \" + str(sys.exc_info()) + \" - \" + repr(e)\n",
    "\n",
    "  # Download earliest version as PDF\n",
    "  if (downloadPapers and (versionsStr != \"NONE\")):\n",
    "    earliestVersionURL = \"http://biorxiv.org\" + earlierVersions[0] + \".full.pdf\"\n",
    "    localDest = \"biorxiv_earliest/\" + bxDOI.replace('/', '_') + \".v1.pdf\"\n",
    "    if (not os.path.isfile(localDest)):\n",
    "      try:\n",
    "        print \"Downloading \" + earliestVersionURL + \" into \" + localDest\n",
    "        urllib.urlretrieve(earliestVersionURL, localDest)\n",
    "        time.sleep(10)\n",
    "      except (urllib.ContentTooShortError, IOError), e:\n",
    "        print \"Error downloading \" + earliestVersionURL + \" \" + str(sys.exc_info()) + \" - \" + repr(e)\n",
    "\n",
    "# THIS IS WHERE THE SCRIPT REALLY BEGINS (I PROMISE)\n",
    "\n",
    "newVersions = 0\n",
    "publishedDOIs = 0\n",
    "\n",
    "with open('biorxiv_dois.txt', 'r') as infile:\n",
    "  for line in infile:\n",
    "    # Reconstruct the link to each article page and process it\n",
    "    lineA = line.split(\"\\t\")\n",
    "    bxDOI = lineA[0].replace('doi.org/', '')\n",
    "    atomPath = lineA[3].replace('.atom', '').replace('/biorxiv/', '')\n",
    "    infoURL = 'http://biorxiv.org/content/' + atomPath + '.article-info'\n",
    "    infoPage = cacheQuery(infoURL)\n",
    "    processArticlePage(infoPage, infoURL, bxDOI)\n",
    "\n",
    "logfile.write(str(newVersions) + \" articles have more than one version\\n\")\n",
    "logfile.write(str(publishedDOIs) + \" articles have publication DOIs\\n\")\n",
    "\n",
    "sortedJournals = sorted(journalCounts.items(), key=operator.itemgetter(1))\n",
    "sortedJournals.reverse()\n",
    "\n",
    "logfile.write(\"JOURNAL COUNTS\\n\")\n",
    "for item in sortedJournals:\n",
    "  logfile.write(str(item[0]) + \" \" + str(item[1]) + \"\\n\")\n",
    "\n",
    "logfile.close()\n",
    "outfile.close()\n",
    "smallout.close()\n",
    "allfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
